{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle \n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "from lyricsgenius import Genius\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"EaFwVotR7TKt9kDiKTnUmjgEciv5vFvoljxqArZ3Sf4pj0BVFEzpTojPqPo7FTFh\"\n",
    "genius = Genius(TOKEN)\n",
    "genius.skip_non_songs = True\n",
    "genius.timeout = 10\n",
    "genius.retries = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics have already been created for this artist\n"
     ]
    }
   ],
   "source": [
    "def check_artist_path(artist, override = False):\n",
    "    artist_f_id = '-'.join(artist.lower().split(' '))\n",
    "    if os.path.exists(f'../artists/{artist_f_id}.p') and not override:\n",
    "        print(\"Lyrics have already been created for this artist\")\n",
    "        return f'../artists/{artist_f_id}.p'\n",
    "    return None\n",
    "artist_path = check_artist_path('Taylor Swift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_songs(artist):\n",
    "    artist_found = genius.search_artist(artist)\n",
    "    if artist_found.name.lower() != artist.lower():\n",
    "        print(f\"Please note that the artist requested was {artist}\")\n",
    "        print(f\"However, the artist used to find search results is {artist_found.name}\")\n",
    "        print(f\"If this seems incorrect, please go back to source code and review!\")\n",
    "    model_name = artist_found.url[artist_found.url.rfind('/') + 1:].lower()\n",
    "    print(artist_found.id)\n",
    "    return artist_found, model_name\n",
    "# ls, model_name = get_all_songs('Laura Stevenson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_songs(artist_found):\n",
    "    song_list = artist_found.songs\n",
    "    print(len(song_list))\n",
    "    artist_found.songs = [song for song in song_list if artist_found.name.lower() in song.primary_artist.name.lower()]\n",
    "    print(len(song_list))\n",
    "    return artist_found\n",
    "# ls2 = limit_songs(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_song_name(name):\n",
    "    a = re.sub(r'\\([^)]*\\)', '', name)\n",
    "    b = re.sub(r'\\[[^)]*\\]', '', a)\n",
    "    return b.strip().lower().replace('\\u200b','')\n",
    "\n",
    "def clean_lyrics(lyrics):\n",
    "    lyrics = re.sub(r'(\\[.*?\\])*', '', lyrics)\n",
    "    lyrics = re.sub('\\n{2}', '\\n', lyrics)  # Gaps between verses\n",
    "    lyrics = re.sub('\\nYou might also like\\n','', lyrics)\n",
    "    lyrics = re.sub('\\nYou might also like\\n','', lyrics)\n",
    "    lyrics = re.sub(r'(You might also like)([\\S]+)',r'\\g<2>', lyrics)\n",
    "\n",
    "    # Want to get consistent apostrophes to get consistent treatment of contractions\n",
    "    lyrics = str(lyrics).replace(\"'\", \"â€™\")\n",
    "    \n",
    "    lyrics = str(lyrics.strip(\"\\n\"))\n",
    "    lyrics = lyrics.replace(\"EmbedShare URLCopyEmbedCopy\", \"\")\n",
    "    lyrics = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", lyrics)\n",
    "    lyrics = re.sub(r'\\d+$', '', lyrics)\n",
    "    lyrics = str(lyrics).lstrip().rstrip()\n",
    "    lyrics = str(lyrics).replace(\"\\n\\n\", \"\\n\")\n",
    "    lyrics = str(lyrics).replace(\"\\n\\n\", \"\\n\")\n",
    "    lyrics = re.sub(' +', ' ', lyrics)\n",
    "    lyrics = str(lyrics).replace('\"', \"\")\n",
    "    # lyrics = str(lyrics).replace(\"'\", \"\")\n",
    "    lyrics = str(lyrics).replace(\"*\", \"\")\n",
    "\n",
    "    # Remove text at front of lyrics, claiming this is start of lyrics\n",
    "    lyrics = re.sub('(^[\\s\\S]* Lyrics[\\\\n]*)','', lyrics)\n",
    "    lyrics = re.sub('(^[\\s\\S]* Lyrics\\\\n)','', lyrics)\n",
    "    if 'lyrics' in lyrics[:100].lower():\n",
    "        print(\"Lyrics are not properly cleaned!\")\n",
    "    \n",
    "    # Remove some random symbols that represent spaces\n",
    "    lyrics = re.sub('\\u2005',' ', lyrics)\n",
    "    \n",
    "    # Remove non-lyrics text that sometimes appears in lyrics\n",
    "    lyrics = re.sub('\\d*Embed$', '', lyrics)\n",
    "    lyrics = re.sub('(You might also like$)','', lyrics)\n",
    "    if 'You might also like' in lyrics:\n",
    "        print(\"Check out lyrics here...\")\n",
    "        print(lyrics)\n",
    "        print()\n",
    "    return lyrics\n",
    "    \n",
    "def get_lyrics(artist_found):\n",
    "    newDict = {}\n",
    "    for song in artist_found.songs:\n",
    "        title = standardize_song_name(song.title)\n",
    "        if title not in newDict.keys():\n",
    "            newDict[title] = clean_lyrics(song.lyrics)\n",
    "    return newDict\n",
    "# lyric_dict = get_lyrics(ls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pickle(artist, lyric_dict):\n",
    "    artist_f_id = '-'.join(artist.lower().split(' '))\n",
    "    with open(f'../artists/{artist_f_id}.p', 'wb') as handle:\n",
    "        pickle.dump(lyric_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_artist_lyrics(artist, override = False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    -A lyrics dict with\n",
    "        title : lyric \n",
    "      pair for all songs found under the artist\n",
    "    \"\"\"\n",
    "    artist_path = check_artist_path(artist, override = override)\n",
    "    if artist_path and not override:\n",
    "        with open(artist_path, 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "    \n",
    "    print(f\"Getting songs for {artist}...\")\n",
    "    artist_found, model_name = get_all_songs(artist)\n",
    "    print(\"Limiting songs...\")\n",
    "    artist_found = limit_songs(artist_found)\n",
    "\n",
    "    print(\"Extracting and cleaning lryics...\")\n",
    "    lyric_dict = get_lyrics(artist_found)\n",
    "\n",
    "    print(\"Writing to pickle\")\n",
    "    write_pickle(artist, (lyric_dict, model_name) )\n",
    "\n",
    "    return lyric_dict, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prelim_dataset(lyric_dict):\n",
    "    lyrics = [v for k, v in lyric_dict.items()]\n",
    "\n",
    "    my_dataset = Dataset.from_dict({'text' : lyrics})\n",
    "    currLen = len(my_dataset)\n",
    "\n",
    "    train_percentage = 0.85\n",
    "    validation_percentage = 0.15\n",
    "    test_percentage = 00\n",
    "\n",
    "    train, valid , test = np.split(lyrics, [int(currLen*train_percentage), int(currLen*(train_percentage + validation_percentage))])\n",
    "\n",
    "    datasets = DatasetDict(\n",
    "                {\n",
    "                    'train' : Dataset.from_dict({'text': train }),\n",
    "                    'valid' : Dataset.from_dict({'text' : valid}),\n",
    "                    'test' : Dataset.from_dict({'text' : test})\n",
    "                })\n",
    "    return datasets\n",
    "\n",
    "# ls_datasets = create_prelim_dataset(ls_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regroup_text(examples, block_size):\n",
    "    combined = {k : list(itertools.chain.from_iterable(examples[k])) for k in examples.keys()}\n",
    "    combined['input_ids']\n",
    "\n",
    "    combined_size = len(combined['input_ids']) // block_size * block_size\n",
    "    test_arr = [i for i in range(combined_size)]\n",
    "    combined_size // block_size\n",
    "\n",
    "    new_dict = {}\n",
    "    for k, v in combined.items():\n",
    "        val = []\n",
    "        for i in range(combined_size // block_size):\n",
    "            val.append(v[i * block_size : i * block_size + block_size])\n",
    "        new_dict[k] = val\n",
    "\n",
    "    new_dict['labels'] = new_dict['input_ids'].copy()\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "def create_lm_dataset(prelim_dataset):\n",
    "    int_datasets = prelim_dataset.map(tokenize_function, batched = True, remove_columns = ['text'])\n",
    "    # [tokenizer.decode(i) for i in test['train'][0]['input_ids']]\n",
    "\n",
    "    block_size = int(tokenizer.model_max_length / 4)\n",
    "\n",
    "    regroup_texts_fn = partial(regroup_text, block_size = block_size)\n",
    "\n",
    "    lm_datasets = int_datasets.map(\n",
    "                    regroup_texts_fn,\n",
    "                    batched = True,\n",
    "                    batch_size = 100,\n",
    "                    num_proc = 1\n",
    "    )\n",
    "    return lm_datasets\n",
    "\n",
    "# lm_datasets = create_lm_dataset(ls_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(ls_datasets, model_name):\n",
    "    folder_path = f'../models/{model_name}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    new_file = folder_path + '/' + 'datasets.p'\n",
    "\n",
    "    with open(new_file, 'wb') as handle:\n",
    "        pickle.dump(ls_datasets, handle)\n",
    "    return\n",
    "\n",
    "# save_datasets(ls_datasets, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics have already been created for this artist\n"
     ]
    }
   ],
   "source": [
    "# artist = 'Kendrick Lamar'\n",
    "ls_lyrics, model_name = get_and_save_artist_lyrics(artist)\n",
    "# ls_datasets = create_prelim_dataset(ls_lyrics)\n",
    "# lm_datasets = create_lm_dataset(ls_datasets)\n",
    "# save_datasets(lm_datasets, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_new2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e08048f17ff008031266911b66a2ac7278627f8d3490a4b4484f3664274d5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
